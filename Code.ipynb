{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8bb80c388804409acaec3400735eb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1415426377c34d40827a5072f987c016",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_596cfaa0a0eb4cf29e8e8a804c8ed223",
              "IPY_MODEL_a521a68f77ba4c37acc5b45774954008"
            ]
          }
        },
        "1415426377c34d40827a5072f987c016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "596cfaa0a0eb4cf29e8e8a804c8ed223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5fb09b07d7db46019ab7678ced233506",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7627,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7627,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b50d0bfd006a4cceabfb458938dceb60"
          }
        },
        "a521a68f77ba4c37acc5b45774954008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_399e3f9c035748bc84e2ccebf5582d93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7627/7627 [01:30&lt;00:00, 84.22it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1455b2397f144cf2811e7fb76debf0dc"
          }
        },
        "5fb09b07d7db46019ab7678ced233506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b50d0bfd006a4cceabfb458938dceb60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "399e3f9c035748bc84e2ccebf5582d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1455b2397f144cf2811e7fb76debf0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1eca0c663c3141a4bf0d5b78731f3006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1042da0ade544071922f8bb530c2b71a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fcc2c98f426c46b2b4ccb57f33b2e26d",
              "IPY_MODEL_948b2eb85a734ec781511e9325fe5cd9"
            ]
          }
        },
        "1042da0ade544071922f8bb530c2b71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcc2c98f426c46b2b4ccb57f33b2e26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dfee1ad40bf84c30bffa9458001ffe62",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7360,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7360,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_595fe64cc2a54c36be1a2ef871a438f2"
          }
        },
        "948b2eb85a734ec781511e9325fe5cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d0d187cac3248688667d06600effb45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7360/7360 [01:11&lt;00:00, 102.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cab388681e914379aff28830d8a1385d"
          }
        },
        "dfee1ad40bf84c30bffa9458001ffe62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "595fe64cc2a54c36be1a2ef871a438f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d0d187cac3248688667d06600effb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cab388681e914379aff28830d8a1385d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM_x9pmkUtlq",
        "outputId": "1ce66146-b34d-470e-be1b-6d300d2de6b2"
      },
      "source": [
        "# libriaries used are loaded\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Bidirectional, GRU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMD3txkFhZ5h",
        "outputId": "2196302e-e4fa-48f0-dce3-5effffec77cd"
      },
      "source": [
        "# grab dataset from google drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7cadVcPU_Yd"
      },
      "source": [
        "!unzip -q '/content/drive/MyDrive/img_test.zip'\n",
        "!unzip -q '/content/drive/MyDrive/img_train.zip'\n",
        "\n",
        "!cp '/content/drive/MyDrive/test_x.csv' 'test_x.csv'\n",
        "!cp '/content/drive/MyDrive/train_xy.csv' 'train_xy.csv'\n",
        "\n",
        "xy_train_df = pd.read_csv('train_xy.csv')\n",
        "x_test_df = pd.read_csv('test_x.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d8bb80c388804409acaec3400735eb85",
            "1415426377c34d40827a5072f987c016",
            "596cfaa0a0eb4cf29e8e8a804c8ed223",
            "a521a68f77ba4c37acc5b45774954008",
            "5fb09b07d7db46019ab7678ced233506",
            "b50d0bfd006a4cceabfb458938dceb60",
            "399e3f9c035748bc84e2ccebf5582d93",
            "1455b2397f144cf2811e7fb76debf0dc"
          ]
        },
        "id": "smbqTEpElJiM",
        "outputId": "bcb5c449-5ffe-4814-8da8-8df47c82ba59"
      },
      "source": [
        "# preprocess image data\n",
        "\n",
        "def load_image(file):\n",
        "    try:\n",
        "        # open an image file when the path is made from correct path\n",
        "        image = Image.open(\n",
        "            file\n",
        "        # image is converted to 'LA' and resizes the image\n",
        "        ).convert('LA').resize((64, 64))\n",
        "        # create numpy array of image\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        # returns a new array for given shape below, filled with zeroes\n",
        "        arr = np.zeros((64, 64, 2))\n",
        "    return arr\n",
        "\n",
        "\n",
        "# loading images:\n",
        "# loads images that are in the xy_train_df, identifies them using the column 'image'.\n",
        "# tqdm is used to show progress of the loop by predicting the remaining time\n",
        "x_image = np.array([load_image(i) for i in tqdm(xy_train_df.image)])\n",
        "\n",
        "# loading summary: (force convert some of the non-string cell to string)\n",
        "x_text = xy_train_df.summary.astype('str')\n",
        "\n",
        "\n",
        "# So to summerize here we are creating image training set have images that \n",
        "# repectively match those in xy_train summary column\n",
        "\n",
        "# we also create test training set by grabbing the summary column of xy_train"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8bb80c388804409acaec3400735eb85",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=7627.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgN07FLmmy8Z",
        "outputId": "9b868797-4cab-4de1-ad06-e219cbd2a41f"
      },
      "source": [
        "# labels:\n",
        "# creating the target column which is price categories (0,1,2)\n",
        "y_price = xy_train_df.price\n",
        "# categorizes variables into numbers for the column 'type' in xy_train\n",
        "# I think we are saving/using this column to make the problem a multi-target model\n",
        "# even though during prediction we only use 'price' for the submission file\n",
        "y_type = xy_train_df.type.astype('category').cat.codes\n",
        "\n",
        "# sanity checks :) \n",
        "len_price = len(y_price.unique())\n",
        "len_type = len(y_type.unique())\n",
        "print('unique values for price category', len_price, y_price.unique())\n",
        "print('unique values for type category', len_type, y_type.unique())\n",
        "\n",
        "# splitting our image and text data 80/20:\n",
        "\n",
        "x_tr_image, x_vl_image, x_tr_text, x_vl_text, y_tr_price, y_vl_price, y_tr_type, y_vl_type = train_test_split(\n",
        "    x_image, \n",
        "    x_text,\n",
        "    y_price,\n",
        "    y_type,\n",
        "    test_size=0.2)\n",
        "\n",
        "print(np.shape(x_tr_image))\n",
        "print(np.shape(x_vl_image))\n",
        "print(np.shape(y_tr_price))\n",
        "print(np.shape(y_vl_price))\n",
        "print(np.shape(y_tr_type))\n",
        "print(np.shape(y_vl_type))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique values for price category 3 [1 0 2]\n",
            "unique values for type category 24 [ 1 17 22 10 18 20  5  2  8  4 23 13 15 16 14 11 19  0 21  3  6 12  7  9]\n",
            "(6101, 64, 64, 2)\n",
            "(1526, 64, 64, 2)\n",
            "(6101,)\n",
            "(1526,)\n",
            "(6101,)\n",
            "(1526,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39H9Hwk8m2Dm",
        "outputId": "8ed93de1-b235-4f46-ff95-d8d3f51b5640"
      },
      "source": [
        "# maximum number of words from the resulting tokenized data which are to be used\n",
        "vocab_size = 40000\n",
        "max_len = 100\n",
        "\n",
        "\n",
        "# build vocabulary from training set\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "# upadting the internal vocalulary based on the list of text, so it creates the vocabulary\n",
        "# index based on word frequnecy so every word gets a unique interger value so lower integers \n",
        "# mean more frequent word.\n",
        "tokenizer.fit_on_texts(x_tr_text)\n",
        "\n",
        "\n",
        "def _preprocess(list_of_text):\n",
        "    # pads sequence to the same length (all sequences in a list to have the same length), it\n",
        "    # does so by padding 0 in the beggining of each sequence until they have the same length as\n",
        "    # the longest sequence. \n",
        "    return pad_sequences(\n",
        "        # transforms each text in texts to a sequence of integers. It takes each word\n",
        "        # in the text and replaces it with its corresponding integer value from the \n",
        "        # dictionary.\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        # takes in the pre-defined input (100) as maximum length of all sequences.\n",
        "        maxlen=max_len,\n",
        "        # does padding after each sequence\n",
        "        padding='post',\n",
        "    )\n",
        "    \n",
        "\n",
        "# padding is done inside: \n",
        "x_tr_text_id = _preprocess(x_tr_text)\n",
        "x_vl_text_id = _preprocess(x_vl_text)\n",
        "\n",
        "print(x_tr_text_id.shape)\n",
        "print(x_vl_text_id.shape)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6101, 100)\n",
            "(1526, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4RY1NVTnK9e"
      },
      "source": [
        "### ORIGINAL TEMPLATE WITH COMMENTS ADDED\n",
        "\n",
        "# defines an input layer (instantiate a Keras tensor object) and allows for building a model.\n",
        "# batch_shape basically means shape=(100,)\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "### text part\n",
        "\n",
        "# this layer can only be used as the first layer in a model. This is the first hidden layer of a \n",
        "# network and will learn an embedding for all of the words in the trainin dataset. Here we are giving \n",
        "# it input and output integers\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "# computes the mean of elements across dimensions of the vector. It reduced the input variables along\n",
        "# the dimenions given in axis by computing the mean of elements across dimensions in the axis. Here in\n",
        "# the provided code we are reducing the dimensions by one.\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "### image part\n",
        "\n",
        "# first convolution layer\n",
        "# the first parameter is the filter indicating the dimensionality of the output space.\n",
        "# next parameter is the kernel_size which specifies the heigh and width of the 2D filter.\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "# Max pooling layer to downsample\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "# flattening the array of pixels\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "\n",
        "### fusion:\n",
        "# concatenates tensors along one dimension (axis)\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "        'type': p_type,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'price': 0.5,\n",
        "        'type': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pucDYoGvip8j"
      },
      "source": [
        "### ATTENTION LAYER ADDED\n",
        "\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "embedded = tf.keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "\n",
        "# Query encoding of shape [batch_size, Tq, filters].\n",
        "query_seq_encoding = Bidirectional(GRU(units = 100))(embedded)\n",
        "# Value encoding of shape [batch_size, Tv, filters].\n",
        "value_seq_encoding = Bidirectional(GRU(units = 100))(embedded)\n",
        "\n",
        "# Query-value attention of shape [batch_size, Tq, filters].\n",
        "query_value_attention_seq = tf.keras.layers.Attention()([query_seq_encoding, value_seq_encoding])\n",
        "atten = tf.keras.layers.Concatenate()([query_seq_encoding, query_value_attention_seq])\n",
        "averaged = atten\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# image part\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "\n",
        "# fusion:\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "        'type': p_type,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'price': 0.5,\n",
        "        'type': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvDF11zuFNli",
        "outputId": "de112af9-861f-4636-8b95-1f8788a6ddad"
      },
      "source": [
        "### CUSTOMIZING IMAGE CONVOLUTIONAL LAYERS\n",
        "\n",
        "\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part\n",
        "cov = Conv2D(32, (16, 16), padding='same', activation='relu')(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "\n",
        "cov2 = Conv2D(64, (16, 16), padding='same', activation='relu')(pl)\n",
        "p2 = MaxPool2D((16, 16), padding='same')(cov2)\n",
        "flattened = Flatten()(p2)\n",
        "\n",
        "\n",
        "fc4 = Dense(84)(flattened)\n",
        "fc5 = Dropout(0.5)(fc4)\n",
        "fc6 = Dense(84)(fc5)\n",
        "fc7 = Dropout(0.5)(fc6)\n",
        "\n",
        "\n",
        "# fusion:\n",
        "fused = tf.concat([averaged, fc7], axis=-1)\n",
        "\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "        'type': p_type,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'price': 0.5,\n",
        "        'type': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 64, 64, 2)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 32)   16416       input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 4, 4, 32)     0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 4, 4, 64)     524352      max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 64)           0           max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 84)           5460        flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 84)           0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, 100, 100)     4000000     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 84)           7140        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean_12 (TensorFlow [(None, 100)]        0           embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 84)           0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_5 (TensorFlo [(None, 184)]        0           tf_op_layer_Mean_12[0][0]        \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "price (Dense)                   (None, 3)            555         tf_op_layer_concat_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "type (Dense)                    (None, 24)           4440        tf_op_layer_concat_5[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 4,558,363\n",
            "Trainable params: 4,558,363\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viG-KFXuIK4z",
        "outputId": "be64dee1-fedd-4869-90cd-008ed19a955f"
      },
      "source": [
        "### FUSION LAYER\n",
        "\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "embedded = keras.layers.Embedding(vocab_size, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "fc4 = Dense(64)(flattened)\n",
        "fc5 = Dropout(0.5)(fc4)\n",
        "fc6 = Dense(100)(fc5)\n",
        "fc7 = Dropout(0.5)(fc6)\n",
        "\n",
        "\n",
        "# fusion:\n",
        "fused = tf.stack([averaged, fc7],axis=-1)\n",
        "flattened2 = Flatten()(fused)\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(flattened2)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(flattened2)\n",
        "\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "        'type': p_type,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'price': 0.5,\n",
        "        'type': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           [(None, 64, 64, 2)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 49, 49, 32)   16416       input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 3, 3, 32)     0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 288)          0           max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           18496       flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_25 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 64)           0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 100, 100)     4000000     input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 100)          6500        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean_11 (TensorFlow [(None, 100)]        0           embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 100)          0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_stack (TensorFlowOp [(None, 100, 2)]     0           tf_op_layer_Mean_11[0][0]        \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 200)          0           tf_op_layer_stack[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "price (Dense)                   (None, 3)            603         flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "type (Dense)                    (None, 24)           4824        flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,046,839\n",
            "Trainable params: 4,046,839\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-oYKS5GboxS",
        "outputId": "f8a5dd55-9957-453a-b5fe-60f145bcb902"
      },
      "source": [
        "### ADVANCDE TOKENIZER\n",
        "\n",
        "vocab_size = 15480 # maxsize limit hit for spm\n",
        "max_len = 100\n",
        "\n",
        "# spm wants a file input so I'm saving the data as a csv here\n",
        "df_x_text= pd.DataFrame(x_tr_text)\n",
        "df_x_text.to_csv('spm.train.set.txt')\n",
        "\n",
        "# training on the data file which creates and saves a model file called 'tokenmodel'\n",
        "spm.SentencePieceTrainer.train(input='spm.train.set.txt', model_prefix='tokenmodel', vocab_size=vocab_size)\n",
        "# uses the file created in the last step to \"tokenize\"\n",
        "tokenizer = spm.SentencePieceProcessor(model_file='tokenmodel.model')\n",
        "\n",
        "\n",
        "def _preprocess(list_of_text):\n",
        "  encoded_list = []\n",
        "  for text in list_of_text:\n",
        "    encoded_text = tokenizer.encode(text, enable_sampling=True, alpha=0.1, nbest_size=-1)\n",
        "    encoded_list.append(encoded_text)\n",
        "\n",
        "  return pad_sequences(\n",
        "      encoded_list,\n",
        "      maxlen=max_len,\n",
        "      padding='post'    )\n",
        "    \n",
        "\n",
        "# padding is done inside: \n",
        "x_tr_text_id = _preprocess(x_tr_text)\n",
        "x_vl_text_id = _preprocess(x_vl_text)\n",
        "\n",
        "print(x_tr_text_id.shape)\n",
        "print(x_vl_text_id.shape)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6101, 100)\n",
            "(1526, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUAPhoW5xeCf",
        "outputId": "d626684d-fc04-4957-ba76-a5ef4664b573"
      },
      "source": [
        "x_tr_text.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6101,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYTVi-lTnR4S",
        "outputId": "5f72d8cb-f1a0-4597-9507-e319d60eabb1"
      },
      "source": [
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_tr_text_id,\n",
        "        'image': x_tr_image\n",
        "    },\n",
        "    y={\n",
        "        'price': y_tr_price,\n",
        "        'type': y_tr_type,\n",
        "    },\n",
        "    epochs=40,\n",
        "    batch_size=50,\n",
        "    validation_data=(\n",
        "        {\n",
        "            'summary': x_vl_text_id,\n",
        "            'image': x_vl_image\n",
        "         }, \n",
        "        {\n",
        "            'price': y_vl_price,\n",
        "            'type': y_vl_type,\n",
        "        }\n",
        "    ),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=10, restore_best_weights= True )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "123/123 [==============================] - 5s 44ms/step - loss: 1.9403 - price_loss: 1.6964 - type_loss: 2.1842 - price_sparse_categorical_accuracy: 0.5489 - type_sparse_categorical_accuracy: 0.7199 - val_loss: 0.9601 - val_price_loss: 0.8450 - val_type_loss: 1.0752 - val_price_sparse_categorical_accuracy: 0.5911 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 2/40\n",
            "123/123 [==============================] - 5s 42ms/step - loss: 0.9629 - price_loss: 0.8461 - type_loss: 1.0797 - price_sparse_categorical_accuracy: 0.6224 - type_sparse_categorical_accuracy: 0.7554 - val_loss: 0.9208 - val_price_loss: 0.8340 - val_type_loss: 1.0076 - val_price_sparse_categorical_accuracy: 0.5944 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 3/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.8905 - price_loss: 0.7910 - type_loss: 0.9900 - price_sparse_categorical_accuracy: 0.6360 - type_sparse_categorical_accuracy: 0.7556 - val_loss: 0.8967 - val_price_loss: 0.8201 - val_type_loss: 0.9734 - val_price_sparse_categorical_accuracy: 0.5963 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 4/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.8591 - price_loss: 0.7646 - type_loss: 0.9535 - price_sparse_categorical_accuracy: 0.6465 - type_sparse_categorical_accuracy: 0.7556 - val_loss: 0.8755 - val_price_loss: 0.7941 - val_type_loss: 0.9568 - val_price_sparse_categorical_accuracy: 0.6147 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 5/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.8321 - price_loss: 0.7383 - type_loss: 0.9259 - price_sparse_categorical_accuracy: 0.6614 - type_sparse_categorical_accuracy: 0.7556 - val_loss: 0.8616 - val_price_loss: 0.7776 - val_type_loss: 0.9456 - val_price_sparse_categorical_accuracy: 0.6311 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 6/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.8070 - price_loss: 0.7100 - type_loss: 0.9040 - price_sparse_categorical_accuracy: 0.6851 - type_sparse_categorical_accuracy: 0.7556 - val_loss: 0.8485 - val_price_loss: 0.7622 - val_type_loss: 0.9349 - val_price_sparse_categorical_accuracy: 0.6540 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 7/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.7826 - price_loss: 0.6848 - type_loss: 0.8804 - price_sparse_categorical_accuracy: 0.7002 - type_sparse_categorical_accuracy: 0.7554 - val_loss: 0.8382 - val_price_loss: 0.7518 - val_type_loss: 0.9247 - val_price_sparse_categorical_accuracy: 0.6592 - val_type_sparse_categorical_accuracy: 0.7575\n",
            "Epoch 8/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.7590 - price_loss: 0.6609 - type_loss: 0.8570 - price_sparse_categorical_accuracy: 0.7192 - type_sparse_categorical_accuracy: 0.7581 - val_loss: 0.8331 - val_price_loss: 0.7513 - val_type_loss: 0.9148 - val_price_sparse_categorical_accuracy: 0.6488 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 9/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.7367 - price_loss: 0.6391 - type_loss: 0.8344 - price_sparse_categorical_accuracy: 0.7261 - type_sparse_categorical_accuracy: 0.7594 - val_loss: 0.8214 - val_price_loss: 0.7372 - val_type_loss: 0.9055 - val_price_sparse_categorical_accuracy: 0.6586 - val_type_sparse_categorical_accuracy: 0.7602\n",
            "Epoch 10/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.7165 - price_loss: 0.6210 - type_loss: 0.8120 - price_sparse_categorical_accuracy: 0.7415 - type_sparse_categorical_accuracy: 0.7630 - val_loss: 0.8161 - val_price_loss: 0.7358 - val_type_loss: 0.8963 - val_price_sparse_categorical_accuracy: 0.6612 - val_type_sparse_categorical_accuracy: 0.7602\n",
            "Epoch 11/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.6955 - price_loss: 0.6014 - type_loss: 0.7895 - price_sparse_categorical_accuracy: 0.7481 - type_sparse_categorical_accuracy: 0.7684 - val_loss: 0.8096 - val_price_loss: 0.7320 - val_type_loss: 0.8872 - val_price_sparse_categorical_accuracy: 0.6638 - val_type_sparse_categorical_accuracy: 0.7602\n",
            "Epoch 12/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.6752 - price_loss: 0.5837 - type_loss: 0.7667 - price_sparse_categorical_accuracy: 0.7553 - type_sparse_categorical_accuracy: 0.7710 - val_loss: 0.8041 - val_price_loss: 0.7287 - val_type_loss: 0.8795 - val_price_sparse_categorical_accuracy: 0.6658 - val_type_sparse_categorical_accuracy: 0.7602\n",
            "Epoch 13/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.6552 - price_loss: 0.5665 - type_loss: 0.7440 - price_sparse_categorical_accuracy: 0.7664 - type_sparse_categorical_accuracy: 0.7740 - val_loss: 0.8051 - val_price_loss: 0.7371 - val_type_loss: 0.8731 - val_price_sparse_categorical_accuracy: 0.6664 - val_type_sparse_categorical_accuracy: 0.7608\n",
            "Epoch 14/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.6364 - price_loss: 0.5506 - type_loss: 0.7222 - price_sparse_categorical_accuracy: 0.7738 - type_sparse_categorical_accuracy: 0.7786 - val_loss: 0.7945 - val_price_loss: 0.7261 - val_type_loss: 0.8629 - val_price_sparse_categorical_accuracy: 0.6691 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 15/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.6177 - price_loss: 0.5350 - type_loss: 0.7003 - price_sparse_categorical_accuracy: 0.7822 - type_sparse_categorical_accuracy: 0.7874 - val_loss: 0.7927 - val_price_loss: 0.7264 - val_type_loss: 0.8590 - val_price_sparse_categorical_accuracy: 0.6697 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 16/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.5991 - price_loss: 0.5191 - type_loss: 0.6790 - price_sparse_categorical_accuracy: 0.7877 - type_sparse_categorical_accuracy: 0.7940 - val_loss: 0.7909 - val_price_loss: 0.7327 - val_type_loss: 0.8491 - val_price_sparse_categorical_accuracy: 0.6684 - val_type_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 17/40\n",
            "123/123 [==============================] - 5s 42ms/step - loss: 0.5812 - price_loss: 0.5048 - type_loss: 0.6575 - price_sparse_categorical_accuracy: 0.7950 - type_sparse_categorical_accuracy: 0.8017 - val_loss: 0.7863 - val_price_loss: 0.7299 - val_type_loss: 0.8427 - val_price_sparse_categorical_accuracy: 0.6678 - val_type_sparse_categorical_accuracy: 0.7674\n",
            "Epoch 18/40\n",
            "123/123 [==============================] - 5s 42ms/step - loss: 0.5629 - price_loss: 0.4894 - type_loss: 0.6363 - price_sparse_categorical_accuracy: 0.8043 - type_sparse_categorical_accuracy: 0.8086 - val_loss: 0.7846 - val_price_loss: 0.7323 - val_type_loss: 0.8369 - val_price_sparse_categorical_accuracy: 0.6710 - val_type_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 19/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.5456 - price_loss: 0.4757 - type_loss: 0.6154 - price_sparse_categorical_accuracy: 0.8112 - type_sparse_categorical_accuracy: 0.8151 - val_loss: 0.7885 - val_price_loss: 0.7443 - val_type_loss: 0.8327 - val_price_sparse_categorical_accuracy: 0.6691 - val_type_sparse_categorical_accuracy: 0.7746\n",
            "Epoch 20/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.5292 - price_loss: 0.4620 - type_loss: 0.5965 - price_sparse_categorical_accuracy: 0.8168 - type_sparse_categorical_accuracy: 0.8210 - val_loss: 0.7807 - val_price_loss: 0.7357 - val_type_loss: 0.8257 - val_price_sparse_categorical_accuracy: 0.6691 - val_type_sparse_categorical_accuracy: 0.7693\n",
            "Epoch 21/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.5148 - price_loss: 0.4492 - type_loss: 0.5804 - price_sparse_categorical_accuracy: 0.8241 - type_sparse_categorical_accuracy: 0.8300 - val_loss: 0.7838 - val_price_loss: 0.7467 - val_type_loss: 0.8209 - val_price_sparse_categorical_accuracy: 0.6730 - val_type_sparse_categorical_accuracy: 0.7779\n",
            "Epoch 22/40\n",
            "123/123 [==============================] - 5s 41ms/step - loss: 0.4964 - price_loss: 0.4355 - type_loss: 0.5572 - price_sparse_categorical_accuracy: 0.8348 - type_sparse_categorical_accuracy: 0.8379 - val_loss: 0.7854 - val_price_loss: 0.7529 - val_type_loss: 0.8180 - val_price_sparse_categorical_accuracy: 0.6684 - val_type_sparse_categorical_accuracy: 0.7765\n",
            "Epoch 23/40\n",
            "123/123 [==============================] - 5s 42ms/step - loss: 0.4819 - price_loss: 0.4232 - type_loss: 0.5405 - price_sparse_categorical_accuracy: 0.8381 - type_sparse_categorical_accuracy: 0.8436 - val_loss: 0.7819 - val_price_loss: 0.7499 - val_type_loss: 0.8140 - val_price_sparse_categorical_accuracy: 0.6678 - val_type_sparse_categorical_accuracy: 0.7779\n",
            "Epoch 24/40\n",
            "123/123 [==============================] - 5s 43ms/step - loss: 0.4677 - price_loss: 0.4119 - type_loss: 0.5235 - price_sparse_categorical_accuracy: 0.8440 - type_sparse_categorical_accuracy: 0.8507 - val_loss: 0.7844 - val_price_loss: 0.7558 - val_type_loss: 0.8131 - val_price_sparse_categorical_accuracy: 0.6671 - val_type_sparse_categorical_accuracy: 0.7746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1eca0c663c3141a4bf0d5b78731f3006",
            "1042da0ade544071922f8bb530c2b71a",
            "fcc2c98f426c46b2b4ccb57f33b2e26d",
            "948b2eb85a734ec781511e9325fe5cd9",
            "dfee1ad40bf84c30bffa9458001ffe62",
            "595fe64cc2a54c36be1a2ef871a438f2",
            "7d0d187cac3248688667d06600effb45",
            "cab388681e914379aff28830d8a1385d"
          ]
        },
        "id": "pFXRHKhanbM1",
        "outputId": "f2750eeb-e8a7-4972-9d27-8919f2a72629"
      },
      "source": [
        "# preprocess test data\n",
        "x_test_summary = _preprocess(x_test_df.summary.astype(str))\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eca0c663c3141a4bf0d5b78731f3006",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=7360.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEYk8toTndjc",
        "outputId": "03047241-e198-4c06-9459-4144e56eb590"
      },
      "source": [
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_summary,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "# only predict for the price column\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n",
        "\n",
        "pd.DataFrame(\n",
        "    {'id': x_test_df.id,\n",
        "     'price': price_category_predicted}).to_csv('conv2D_best_weights.spm.csv', index=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.7471423  0.20665012 0.04620761]\n",
            " [0.65297514 0.29617396 0.05085089]\n",
            " [0.6861692  0.2708084  0.04302243]\n",
            " ...\n",
            " [0.68382627 0.2691224  0.04705141]\n",
            " [0.3892078  0.46857643 0.14221576]\n",
            " [0.8221707  0.1432627  0.03456659]]\n",
            "[0 0 0 ... 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}